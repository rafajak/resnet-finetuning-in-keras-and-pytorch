{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1.8.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib.eager.python import tfe\n",
    "tfe.enable_eager_execution()\n",
    "from tensorflow import keras\n",
    "# from tensorflow import keras.preprocessing.imageImageDataGenerator \n",
    "# import tf.contrib.keras.preprocessing.image.ImageDataGenerator as ImageDataGenerator\n",
    "# # keras.preprocessing.image.ImageDataGenerator\n",
    "# import keras\n",
    "# from keras import layers\n",
    "# from keras.preprocessing import image # keras.preprocessing.image\n",
    "# from keras.preprocessing.image import ImageDataGenerator\n",
    "# from keras.applications import ResNet50\n",
    "# from keras.applications.resnet50 import preprocess_input\n",
    "# from keras.callbacks import ModelCheckpoint\n",
    "# from keras.models import load_model #  keras.models.load_model\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import os, shutil\n",
    "from livelossplot import PlotLossesKeras\n",
    "import numpy as np\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.python.client import device_lib\n",
    "\n",
    "# device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '3' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindir ='/mnt/ml-team/homes/rafal.jakubanis/blogpost2-resnet50/data_small/train'\n",
    "valdir = '/mnt/ml-team/homes/rafal.jakubanis/blogpost2-resnet50/data_small/validation'\n",
    "\n",
    "batch_size = 32 #32\n",
    "img_size = 224\n",
    "epochs = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Keras data generators "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#todo: figure out how to add preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n",
      "Found 1000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = keras.preprocessing.image.ImageDataGenerator(\n",
    "#     preprocessing_function=keras.applications.resnet50.preprocess_input,\n",
    "    shear_range=10,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True)\n",
    "\n",
    "validation_datagen = keras.preprocessing.image.ImageDataGenerator()\n",
    "#     preprocessing_function=keras.applications.resnet50.preprocess_input)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    traindir,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary',\n",
    "    target_size=(img_size,img_size))\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "    valdir,\n",
    "    shuffle=False,\n",
    "    class_mode='binary',\n",
    "    target_size=(img_size,img_size))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import pre-trained ResNet50\n",
    "\n",
    "Time to load the pre-trained ResNet model. We'll freeze all layers, so there's no backpropagation of gradients through the ResNet50 layers (this saves us A LOT of time and resources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.set_learning_phase(0) # see https://github.com/keras-team/keras/issues/9214  and https://github.com/keras-team/keras/pull/9965\n",
    "\n",
    "conv_base = keras.applications.ResNet50(\n",
    "    include_top=False,\n",
    "    weights='imagenet')\n",
    "\n",
    "# freeze layers in resnet50\n",
    "for layer in conv_base.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stack trainable, fully-connected (dense) layers on top of it\n",
    "\n",
    "These are the layers, which will have randomly initiated weights: these are the ones we'll actually train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = conv_base.output\n",
    "keras.backend.set_learning_phase(1)\n",
    "model = keras.layers.GlobalAveragePooling2D()(model)\n",
    "\n",
    "# add fc layers\n",
    "model = keras.layers.Dense(128)(model) \n",
    "predictions = keras.layers.Dense(1, activation='sigmoid')(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare model checkpointing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Model(conv_base.input, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint_dir = \"./output\"\n",
    "\n",
    "if not os.path.exists(model_checkpoint_dir):\n",
    "    os.mkdir(model_checkpoint_dir)\n",
    "    print('Created directory: ', model_checkpoint_dir)\n",
    "\n",
    "model_checkpoint_dir = model_checkpoint_dir + \"/best_model.hdf5\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras API allows us to easily save the model architecture at the stage it reaches the best validation accuracy (or any metric we want to optimize for). The saving is done through ModelCheckpoint callback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_save_best = keras.callbacks.ModelCheckpoint(model_checkpoint_dir, monitor='val_acc',\n",
    "verbose=1, save_best_only=True, save_weights_only=False, mode='max', period=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.trainable_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile and train the model (A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If eager mode wasn't enabled, we could compile train the model using the standard Keras API, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# optimizer = keras.optimizers.RMSprop()\n",
    "\n",
    "# model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But since we want to see how eager execution works, we'll do it differently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that compiling a tf.keras model while tfe.enable_eager_execution() results in: \n",
    "\"ValueError: Only TF native optimizers are supported in Eager mode.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# history = model.fit_generator(generator=train_generator,\n",
    "#                     epochs=epochs,\n",
    "#                     validation_data=validation_generator, \n",
    "#                     steps_per_epoch = train_generator.n // batch_size,\n",
    "#                     validation_steps = validation_generator.n // batch_size,\n",
    "#                     initial_epoch=0,\n",
    "#                     callbacks=[checkpoint_save_best,])\n",
    "#                     #          plot_losses])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model (B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorflow eager execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "# optimizer = tf.keras.optimizers.RMSprop()\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(model, inputs, labels):\n",
    "    prediction = model(inputs)\n",
    "    \n",
    "    return tf.losses.sigmoid_cross_entropy(logits=prediction, multi_class_labels=tf.expand_dims(labels, dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad(model, inputs, targets):\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss_value = loss(model, inputs, targets)\n",
    "    return tape.gradient(loss_value, model.variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print the loss on a single batch to see if it works so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial loss: 0.661\n"
     ]
    }
   ],
   "source": [
    "#test forward prop\n",
    "with tf.device(\"/gpu:0\"):\n",
    "    x, y = iter(train_generator).next()\n",
    "    print(\"Initial loss: {:.3f}\".format(loss(model, x, y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#there's something wrong with the below- it's suspiciously slow, compared to the pure keras version..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 0000: 0.660\n",
      "Loss at step 0001: 0.661\n",
      "Loss at step 0002: 0.661\n",
      "Loss at step 0003: 0.660\n",
      "Loss at step 0004: 0.661\n",
      "Loss at step 0005: 0.660\n",
      "Loss at step 0006: 0.660\n",
      "Loss at step 0007: 0.659\n",
      "Loss at step 0008: 0.659\n",
      "Loss at step 0009: 0.659\n",
      "Loss at step 0010: 0.657\n",
      "Loss at step 0011: 0.658\n",
      "Loss at step 0012: 0.657\n",
      "Loss at step 0013: 0.657\n",
      "Loss at step 0014: 0.657\n",
      "Loss at step 0015: 0.657\n",
      "Loss at step 0016: 0.656\n",
      "Loss at step 0017: 0.655\n",
      "Loss at step 0018: 0.654\n",
      "Loss at step 0019: 0.652\n",
      "Loss at step 0020: 0.652\n",
      "Loss at step 0021: 0.652\n",
      "Loss at step 0022: 0.651\n",
      "Loss at step 0023: 0.650\n",
      "Loss at step 0024: 0.648\n",
      "Loss at step 0025: 0.648\n",
      "Loss at step 0026: 0.648\n",
      "Loss at step 0027: 0.645\n",
      "Loss at step 0028: 0.643\n",
      "Loss at step 0029: 0.641\n",
      "Loss at step 0030: 0.640\n",
      "Loss at step 0031: 0.637\n",
      "Loss at step 0032: 0.637\n",
      "Loss at step 0033: 0.632\n",
      "Loss at step 0034: 0.633\n",
      "Loss at step 0035: 0.627\n",
      "Loss at step 0036: 0.624\n",
      "Loss at step 0037: 0.625\n",
      "Loss at step 0038: 0.618\n",
      "Loss at step 0039: 0.614\n",
      "Loss at step 0040: 0.609\n",
      "Loss at step 0041: 0.606\n",
      "Loss at step 0042: 0.602\n",
      "Loss at step 0043: 0.600\n",
      "Loss at step 0044: 0.592\n",
      "Loss at step 0045: 0.577\n",
      "Loss at step 0046: 0.565\n",
      "Loss at step 0047: 0.560\n",
      "Loss at step 0048: 0.557\n",
      "Loss at step 0049: 0.563\n",
      "Loss at step 0050: 0.561\n",
      "Loss at step 0051: 0.564\n",
      "Loss at step 0052: 0.561\n",
      "Loss at step 0053: 0.557\n",
      "Loss at step 0054: 0.545\n",
      "Loss at step 0055: 0.536\n",
      "Loss at step 0056: 0.529\n",
      "Loss at step 0057: 0.526\n",
      "Loss at step 0058: 0.525\n",
      "Loss at step 0059: 0.521\n",
      "Loss at step 0060: 0.535\n",
      "Loss at step 0061: 0.525\n",
      "Loss at step 0062: 0.533\n",
      "Loss at step 0063: 0.512\n",
      "Loss at step 0064: 0.514\n",
      "Loss at step 0065: 0.507\n",
      "Loss at step 0066: 0.524\n",
      "Loss at step 0067: 0.509\n",
      "Loss at step 0068: 0.492\n",
      "Loss at step 0069: 0.493\n",
      "Loss at step 0070: 0.496\n",
      "Loss at step 0071: 0.491\n",
      "Loss at step 0072: 0.510\n",
      "Loss at step 0073: 0.499\n",
      "Loss at step 0074: 0.503\n",
      "Loss at step 0075: 0.516\n",
      "Loss at step 0076: 0.478\n",
      "Loss at step 0077: 0.497\n",
      "Loss at step 0078: 0.483\n",
      "Loss at step 0079: 0.482\n",
      "Loss at step 0080: 0.480\n",
      "Loss at step 0081: 0.488\n",
      "Loss at step 0082: 0.486\n",
      "Loss at step 0083: 0.472\n",
      "Loss at step 0084: 0.484\n",
      "Loss at step 0085: 0.504\n",
      "Loss at step 0086: 0.510\n",
      "Loss at step 0087: 0.488\n",
      "Loss at step 0088: 0.506\n",
      "Loss at step 0089: 0.529\n",
      "Loss at step 0090: 0.489\n",
      "Loss at step 0091: 0.459\n",
      "Loss at step 0092: 0.458\n",
      "Loss at step 0093: 0.466\n",
      "Loss at step 0094: 0.467\n",
      "Loss at step 0095: 0.459\n",
      "Loss at step 0096: 0.475\n",
      "Loss at step 0097: 0.462\n",
      "Loss at step 0098: 0.446\n",
      "Loss at step 0099: 0.448\n",
      "Loss at step 0100: 0.501\n",
      "Loss at step 0101: 0.464\n",
      "Loss at step 0102: 0.482\n",
      "Loss at step 0103: 0.457\n",
      "Loss at step 0104: 0.484\n",
      "Loss at step 0105: 0.489\n",
      "Loss at step 0106: 0.460\n",
      "Loss at step 0107: 0.499\n",
      "Loss at step 0108: 0.465\n",
      "Loss at step 0109: 0.459\n",
      "Loss at step 0110: 0.444\n",
      "Loss at step 0111: 0.446\n",
      "Loss at step 0112: 0.537\n",
      "Loss at step 0113: 0.449\n",
      "Loss at step 0114: 0.479\n",
      "Loss at step 0115: 0.503\n",
      "Loss at step 0116: 0.474\n",
      "Loss at step 0117: 0.449\n",
      "Loss at step 0118: 0.477\n",
      "Loss at step 0119: 0.497\n",
      "Loss at step 0120: 0.486\n",
      "Loss at step 0121: 0.502\n",
      "Loss at step 0122: 0.462\n",
      "Loss at step 0123: 0.460\n",
      "Loss at step 0124: 0.482\n",
      "Loss at step 0125: 0.449\n",
      "Loss at step 0126: 0.576\n",
      "Loss at step 0127: 0.518\n",
      "Loss at step 0128: 0.515\n",
      "Loss at step 0129: 0.535\n",
      "Loss at step 0130: 0.505\n",
      "Loss at step 0131: 0.508\n",
      "Loss at step 0132: 0.555\n",
      "Loss at step 0133: 0.554\n",
      "Loss at step 0134: 0.605\n",
      "Loss at step 0135: 0.587\n",
      "Loss at step 0136: 0.549\n",
      "Loss at step 0137: 0.551\n",
      "Loss at step 0138: 0.640\n",
      "Loss at step 0139: 0.617\n",
      "Loss at step 0140: 0.578\n",
      "Loss at step 0141: 0.575\n",
      "Loss at step 0142: 0.622\n",
      "Loss at step 0143: 0.546\n",
      "Loss at step 0144: 0.548\n",
      "Loss at step 0145: 0.582\n",
      "Loss at step 0146: 0.590\n",
      "Loss at step 0147: 0.598\n",
      "Loss at step 0148: 0.530\n",
      "Loss at step 0149: 0.527\n",
      "Loss at step 0150: 0.606\n",
      "Loss at step 0151: 0.652\n",
      "Loss at step 0152: 0.565\n",
      "Loss at step 0153: 0.544\n",
      "Loss at step 0154: 0.605\n",
      "Loss at step 0155: 0.598\n",
      "Loss at step 0156: 0.589\n",
      "Loss at step 0157: 0.616\n",
      "Loss at step 0158: 0.687\n",
      "Loss at step 0159: 0.551\n",
      "Loss at step 0160: 0.615\n",
      "Loss at step 0161: 0.569\n",
      "Loss at step 0162: 0.599\n",
      "Loss at step 0163: 0.616\n",
      "Loss at step 0164: 0.599\n",
      "Loss at step 0165: 0.596\n",
      "Loss at step 0166: 0.577\n",
      "Loss at step 0167: 0.568\n",
      "Loss at step 0168: 0.576\n",
      "Loss at step 0169: 0.575\n",
      "Loss at step 0170: 0.581\n",
      "Loss at step 0171: 0.616\n",
      "Loss at step 0172: 0.597\n",
      "Loss at step 0173: 0.590\n",
      "Loss at step 0174: 0.607\n",
      "Loss at step 0175: 0.615\n",
      "Loss at step 0176: 0.650\n",
      "Loss at step 0177: 0.642\n",
      "Loss at step 0178: 0.561\n",
      "Loss at step 0179: 0.668\n",
      "Loss at step 0180: 0.619\n",
      "Loss at step 0181: 0.633\n",
      "Loss at step 0182: 0.622\n",
      "Loss at step 0183: 0.632\n",
      "Loss at step 0184: 0.659\n",
      "Loss at step 0185: 0.602\n",
      "Loss at step 0186: 0.598\n",
      "Loss at step 0187: 0.671\n",
      "Loss at step 0188: 0.654\n",
      "Loss at step 0189: 0.655\n",
      "Loss at step 0190: 0.655\n",
      "Loss at step 0191: 0.681\n",
      "Loss at step 0192: 0.662\n",
      "Loss at step 0193: 0.670\n",
      "Loss at step 0194: 0.683\n",
      "Loss at step 0195: 0.679\n",
      "Loss at step 0196: 0.716\n",
      "Loss at step 0197: 0.722\n",
      "Loss at step 0198: 0.706\n",
      "Loss at step 0199: 0.688\n",
      "Loss at step 0200: 0.691\n",
      "Loss at step 0201: 0.689\n",
      "Loss at step 0202: 0.691\n",
      "Loss at step 0203: 0.691\n",
      "Loss at step 0204: 0.691\n",
      "Loss at step 0205: 0.691\n",
      "Loss at step 0206: 0.693\n",
      "Loss at step 0207: 0.675\n",
      "Loss at step 0208: 0.678\n",
      "Loss at step 0209: 0.688\n",
      "Loss at step 0210: 0.697\n",
      "Loss at step 0211: 0.697\n",
      "Loss at step 0212: 0.693\n",
      "Loss at step 0213: 0.693\n",
      "Loss at step 0214: 0.693\n",
      "Loss at step 0215: 0.695\n",
      "Loss at step 0216: 0.695\n",
      "Loss at step 0217: 0.695\n",
      "Loss at step 0218: 0.695\n",
      "Loss at step 0219: 0.696\n",
      "Loss at step 0220: 0.698\n",
      "Loss at step 0221: 0.697\n",
      "Loss at step 0222: 0.695\n",
      "Loss at step 0223: 0.695\n",
      "Loss at step 0224: 0.694\n",
      "Loss at step 0225: 0.694\n",
      "Loss at step 0226: 0.694\n",
      "Loss at step 0227: 0.694\n",
      "Loss at step 0228: 0.694\n",
      "Loss at step 0229: 0.691\n",
      "Loss at step 0230: 0.675\n",
      "Loss at step 0231: 0.673\n",
      "Loss at step 0232: 0.688\n",
      "Loss at step 0233: 0.651\n",
      "Loss at step 0234: 0.657\n",
      "Loss at step 0235: 0.668\n",
      "Loss at step 0236: 0.650\n",
      "Loss at step 0237: 0.684\n",
      "Loss at step 0238: 0.661\n",
      "Loss at step 0239: 0.674\n",
      "Loss at step 0240: 0.672\n",
      "Loss at step 0241: 0.632\n",
      "Loss at step 0242: 0.623\n",
      "Loss at step 0243: 0.673\n",
      "Loss at step 0244: 0.658\n",
      "Loss at step 0245: 0.662\n",
      "Loss at step 0246: 0.671\n",
      "Loss at step 0247: 0.679\n",
      "Loss at step 0248: 0.646\n",
      "Loss at step 0249: 0.590\n",
      "Loss at step 0250: 0.644\n",
      "Loss at step 0251: 0.721\n",
      "Loss at step 0252: 0.677\n",
      "Loss at step 0253: 0.702\n",
      "Loss at step 0254: 0.709\n",
      "Loss at step 0255: 0.683\n",
      "Loss at step 0256: 0.649\n",
      "Loss at step 0257: 0.598\n",
      "Loss at step 0258: 0.663\n",
      "Loss at step 0259: 0.632\n",
      "Loss at step 0260: 0.689\n",
      "Loss at step 0261: 0.691\n",
      "Loss at step 0262: 0.687\n",
      "Loss at step 0263: 0.697\n",
      "Loss at step 0264: 0.690\n",
      "Loss at step 0265: 0.693\n",
      "Loss at step 0266: 0.697\n",
      "Loss at step 0267: 0.699\n",
      "Loss at step 0268: 0.696\n",
      "Loss at step 0269: 0.660\n",
      "Loss at step 0270: 0.657\n",
      "Loss at step 0271: 0.672\n",
      "Loss at step 0272: 0.643\n",
      "Loss at step 0273: 0.645\n",
      "Loss at step 0274: 0.640\n",
      "Loss at step 0275: 0.699\n",
      "Loss at step 0276: 0.647\n",
      "Loss at step 0277: 0.654\n",
      "Loss at step 0278: 0.646\n",
      "Loss at step 0279: 0.636\n",
      "Loss at step 0280: 0.667\n",
      "Loss at step 0281: 0.628\n",
      "Loss at step 0282: 0.665\n",
      "Loss at step 0283: 0.628\n",
      "Loss at step 0284: 0.649\n",
      "Loss at step 0285: 0.656\n",
      "Loss at step 0286: 0.675\n",
      "Loss at step 0287: 0.656\n",
      "Loss at step 0288: 0.596\n",
      "Loss at step 0289: 0.626\n",
      "Loss at step 0290: 0.652\n",
      "Loss at step 0291: 0.625\n",
      "Loss at step 0292: 0.653\n",
      "Loss at step 0293: 0.657\n",
      "Loss at step 0294: 0.657\n",
      "Loss at step 0295: 0.668\n",
      "Loss at step 0296: 0.666\n",
      "Loss at step 0297: 0.635\n",
      "Loss at step 0298: 0.650\n",
      "Loss at step 0299: 0.554\n",
      "Loss at step 0300: 0.662\n",
      "Loss at step 0301: 0.663\n",
      "Loss at step 0302: 0.638\n",
      "Loss at step 0303: 0.664\n",
      "Loss at step 0304: 0.670\n",
      "Loss at step 0305: 0.669\n",
      "Loss at step 0306: 0.671\n",
      "Loss at step 0307: 0.669\n",
      "Loss at step 0308: 0.665\n",
      "Loss at step 0309: 0.657\n",
      "Loss at step 0310: 0.646\n",
      "Loss at step 0311: 0.676\n",
      "Loss at step 0312: 0.647\n",
      "Loss at step 0313: 0.647\n",
      "Loss at step 0314: 0.633\n",
      "Loss at step 0315: 0.624\n",
      "Loss at step 0316: 0.623\n",
      "Loss at step 0317: 0.635\n",
      "Loss at step 0318: 0.635\n",
      "Loss at step 0319: 0.635\n",
      "Loss at step 0320: 0.638\n",
      "Loss at step 0321: 0.632\n",
      "Loss at step 0322: 0.636\n",
      "Loss at step 0323: 0.644\n",
      "Loss at step 0324: 0.642\n",
      "Loss at step 0325: 0.644\n",
      "Loss at step 0326: 0.642\n",
      "Loss at step 0327: 0.640\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 0328: 0.650\n",
      "Loss at step 0329: 0.672\n",
      "Loss at step 0330: 0.667\n",
      "Loss at step 0331: 0.663\n",
      "Loss at step 0332: 0.671\n",
      "Loss at step 0333: 0.655\n",
      "Loss at step 0334: 0.655\n",
      "Loss at step 0335: 0.664\n",
      "Loss at step 0336: 0.668\n",
      "Loss at step 0337: 0.664\n",
      "Loss at step 0338: 0.665\n",
      "Loss at step 0339: 0.656\n",
      "Loss at step 0340: 0.640\n",
      "Loss at step 0341: 0.683\n",
      "Loss at step 0342: 0.667\n",
      "Loss at step 0343: 0.653\n",
      "Loss at step 0344: 0.657\n",
      "Loss at step 0345: 0.646\n",
      "Loss at step 0346: 0.638\n",
      "Loss at step 0347: 0.642\n",
      "Loss at step 0348: 0.655\n",
      "Loss at step 0349: 0.616\n",
      "Loss at step 0350: 0.599\n",
      "Loss at step 0351: 0.615\n",
      "Loss at step 0352: 0.690\n",
      "Loss at step 0353: 0.660\n",
      "Loss at step 0354: 0.658\n",
      "Loss at step 0355: 0.634\n",
      "Loss at step 0356: 0.639\n",
      "Loss at step 0357: 0.637\n",
      "Loss at step 0358: 0.656\n",
      "Loss at step 0359: 0.655\n",
      "Loss at step 0360: 0.655\n",
      "Loss at step 0361: 0.649\n",
      "Loss at step 0362: 0.638\n",
      "Loss at step 0363: 0.636\n",
      "Loss at step 0364: 0.625\n",
      "Loss at step 0365: 0.608\n",
      "Loss at step 0366: 0.599\n",
      "Loss at step 0367: 0.621\n",
      "Loss at step 0368: 0.612\n",
      "Loss at step 0369: 0.607\n",
      "Loss at step 0370: 0.608\n",
      "Loss at step 0371: 0.616\n",
      "Loss at step 0372: 0.622\n",
      "Loss at step 0373: 0.607\n",
      "Loss at step 0374: 0.606\n",
      "Loss at step 0375: 0.625\n",
      "Loss at step 0376: 0.611\n",
      "Loss at step 0377: 0.619\n",
      "Loss at step 0378: 0.621\n",
      "Loss at step 0379: 0.641\n",
      "Loss at step 0380: 0.627\n",
      "Loss at step 0381: 0.615\n",
      "Loss at step 0382: 0.631\n",
      "Loss at step 0383: 0.633\n",
      "Loss at step 0384: 0.647\n",
      "Loss at step 0385: 0.686\n",
      "Loss at step 0386: 0.685\n",
      "Loss at step 0387: 0.693\n",
      "Loss at step 0388: 0.685\n",
      "Loss at step 0389: 0.651\n",
      "Loss at step 0390: 0.640\n",
      "Loss at step 0391: 0.624\n",
      "Loss at step 0392: 0.619\n",
      "Loss at step 0393: 0.627\n",
      "Loss at step 0394: 0.618\n",
      "Loss at step 0395: 0.683\n",
      "Loss at step 0396: 0.651\n",
      "Loss at step 0397: 0.648\n",
      "Loss at step 0398: 0.675\n",
      "Loss at step 0399: 0.638\n",
      "Loss at step 0400: 0.635\n",
      "Loss at step 0401: 0.672\n",
      "Loss at step 0402: 0.679\n",
      "Loss at step 0403: 0.674\n",
      "Loss at step 0404: 0.637\n",
      "Loss at step 0405: 0.635\n",
      "Loss at step 0406: 0.635\n",
      "Loss at step 0407: 0.652\n",
      "Loss at step 0408: 0.626\n",
      "Loss at step 0409: 0.627\n",
      "Loss at step 0410: 0.642\n",
      "Loss at step 0411: 0.635\n",
      "Loss at step 0412: 0.636\n",
      "Loss at step 0413: 0.634\n",
      "Loss at step 0414: 0.626\n",
      "Loss at step 0415: 0.623\n",
      "Loss at step 0416: 0.641\n",
      "Loss at step 0417: 0.633\n",
      "Loss at step 0418: 0.639\n",
      "Loss at step 0419: 0.643\n",
      "Loss at step 0420: 0.647\n",
      "Loss at step 0421: 0.648\n",
      "Loss at step 0422: 0.610\n",
      "Loss at step 0423: 0.624\n",
      "Loss at step 0424: 0.624\n",
      "Loss at step 0425: 0.638\n",
      "Loss at step 0426: 0.631\n",
      "Loss at step 0427: 0.622\n",
      "Loss at step 0428: 0.644\n",
      "Loss at step 0429: 0.640\n",
      "Loss at step 0430: 0.645\n",
      "Loss at step 0431: 0.653\n",
      "Loss at step 0432: 0.657\n",
      "Loss at step 0433: 0.650\n",
      "Loss at step 0434: 0.644\n",
      "Loss at step 0435: 0.647\n",
      "Loss at step 0436: 0.658\n",
      "Loss at step 0437: 0.652\n",
      "Loss at step 0438: 0.652\n",
      "Loss at step 0439: 0.655\n",
      "Loss at step 0440: 0.646\n",
      "Loss at step 0441: 0.646\n",
      "Loss at step 0442: 0.645\n",
      "Loss at step 0443: 0.646\n",
      "Loss at step 0444: 0.650\n",
      "Loss at step 0445: 0.629\n",
      "Loss at step 0446: 0.640\n",
      "Loss at step 0447: 0.666\n",
      "Loss at step 0448: 0.645\n",
      "Loss at step 0449: 0.637\n",
      "Loss at step 0450: 0.657\n",
      "Loss at step 0451: 0.647\n",
      "Loss at step 0452: 0.641\n",
      "Loss at step 0453: 0.638\n",
      "Loss at step 0454: 0.634\n",
      "Loss at step 0455: 0.632\n",
      "Loss at step 0456: 0.630\n",
      "Loss at step 0457: 0.648\n",
      "Loss at step 0458: 0.644\n",
      "Loss at step 0459: 0.643\n",
      "Loss at step 0460: 0.634\n",
      "Loss at step 0461: 0.657\n",
      "Loss at step 0462: 0.662\n",
      "Loss at step 0463: 0.665\n",
      "Loss at step 0464: 0.649\n",
      "Loss at step 0465: 0.638\n",
      "Loss at step 0466: 0.640\n",
      "Loss at step 0467: 0.652\n",
      "Loss at step 0468: 0.625\n",
      "Loss at step 0469: 0.629\n",
      "Loss at step 0470: 0.656\n",
      "Loss at step 0471: 0.632\n",
      "Loss at step 0472: 0.632\n",
      "Loss at step 0473: 0.630\n",
      "Loss at step 0474: 0.616\n",
      "Loss at step 0475: 0.616\n",
      "Loss at step 0476: 0.668\n",
      "Loss at step 0477: 0.638\n",
      "Loss at step 0478: 0.638\n",
      "Loss at step 0479: 0.634\n",
      "Loss at step 0480: 0.673\n",
      "Loss at step 0481: 0.690\n",
      "Loss at step 0482: 0.689\n",
      "Loss at step 0483: 0.701\n",
      "Loss at step 0484: 0.657\n",
      "Loss at step 0485: 0.654\n",
      "Loss at step 0486: 0.681\n",
      "Loss at step 0487: 0.670\n",
      "Loss at step 0488: 0.664\n",
      "Loss at step 0489: 0.651\n",
      "Loss at step 0490: 0.655\n",
      "Loss at step 0491: 0.652\n",
      "Loss at step 0492: 0.665\n",
      "Loss at step 0493: 0.662\n",
      "Loss at step 0494: 0.664\n",
      "Loss at step 0495: 0.657\n",
      "Loss at step 0496: 0.658\n",
      "Loss at step 0497: 0.637\n",
      "Loss at step 0498: 0.658\n",
      "Loss at step 0499: 0.650\n",
      "Loss at step 0500: 0.663\n",
      "Loss at step 0501: 0.657\n",
      "Loss at step 0502: 0.624\n",
      "Loss at step 0503: 0.629\n",
      "Loss at step 0504: 0.651\n",
      "Loss at step 0505: 0.645\n",
      "Loss at step 0506: 0.634\n",
      "Loss at step 0507: 0.638\n",
      "Loss at step 0508: 0.639\n",
      "Loss at step 0509: 0.627\n",
      "Loss at step 0510: 0.625\n",
      "Loss at step 0511: 0.628\n",
      "Loss at step 0512: 0.629\n",
      "Loss at step 0513: 0.630\n",
      "Loss at step 0514: 0.636\n",
      "Loss at step 0515: 0.638\n",
      "Loss at step 0516: 0.641\n",
      "Loss at step 0517: 0.642\n",
      "Loss at step 0518: 0.628\n",
      "Loss at step 0519: 0.616\n",
      "Loss at step 0520: 0.630\n",
      "Loss at step 0521: 0.585\n",
      "Loss at step 0522: 0.629\n",
      "Loss at step 0523: 0.645\n",
      "Loss at step 0524: 0.634\n",
      "Loss at step 0525: 0.637\n",
      "Loss at step 0526: 0.637\n",
      "Loss at step 0527: 0.606\n",
      "Loss at step 0528: 0.613\n",
      "Loss at step 0529: 0.592\n",
      "Loss at step 0530: 0.591\n",
      "Loss at step 0531: 0.593\n",
      "Loss at step 0532: 0.599\n",
      "Loss at step 0533: 0.605\n",
      "Loss at step 0534: 0.602\n",
      "Loss at step 0535: 0.609\n",
      "Loss at step 0536: 0.609\n",
      "Loss at step 0537: 0.635\n",
      "Loss at step 0538: 0.608\n",
      "Loss at step 0539: 0.625\n",
      "Loss at step 0540: 0.604\n",
      "Loss at step 0541: 0.581\n",
      "Loss at step 0542: 0.608\n",
      "Loss at step 0543: 0.615\n",
      "Loss at step 0544: 0.614\n",
      "Loss at step 0545: 0.619\n",
      "Loss at step 0546: 0.605\n",
      "Loss at step 0547: 0.605\n",
      "Loss at step 0548: 0.626\n",
      "Loss at step 0549: 0.598\n",
      "Loss at step 0550: 0.610\n",
      "Loss at step 0551: 0.587\n",
      "Loss at step 0552: 0.584\n",
      "Loss at step 0553: 0.593\n",
      "Loss at step 0554: 0.599\n",
      "Loss at step 0555: 0.619\n",
      "Loss at step 0556: 0.607\n",
      "Loss at step 0557: 0.607\n",
      "Loss at step 0558: 0.608\n",
      "Loss at step 0559: 0.644\n",
      "Loss at step 0560: 0.645\n",
      "Loss at step 0561: 0.644\n",
      "Loss at step 0562: 0.643\n",
      "Loss at step 0563: 0.645\n",
      "Loss at step 0564: 0.642\n",
      "Loss at step 0565: 0.628\n",
      "Loss at step 0566: 0.622\n",
      "Loss at step 0567: 0.621\n",
      "Loss at step 0568: 0.616\n",
      "Loss at step 0569: 0.639\n",
      "Loss at step 0570: 0.637\n",
      "Loss at step 0571: 0.644\n",
      "Loss at step 0572: 0.649\n",
      "Loss at step 0573: 0.649\n",
      "Loss at step 0574: 0.648\n",
      "Loss at step 0575: 0.626\n",
      "Loss at step 0576: 0.627\n",
      "Loss at step 0577: 0.627\n",
      "Loss at step 0578: 0.650\n",
      "Loss at step 0579: 0.672\n",
      "Loss at step 0580: 0.654\n",
      "Loss at step 0581: 0.653\n",
      "Loss at step 0582: 0.663\n",
      "Loss at step 0583: 0.666\n",
      "Loss at step 0584: 0.674\n",
      "Loss at step 0585: 0.674\n",
      "Loss at step 0586: 0.641\n",
      "Loss at step 0587: 0.637\n",
      "Loss at step 0588: 0.638\n",
      "Loss at step 0589: 0.666\n",
      "Loss at step 0590: 0.673\n",
      "Loss at step 0591: 0.667\n",
      "Loss at step 0592: 0.673\n",
      "Loss at step 0593: 0.669\n",
      "Loss at step 0594: 0.655\n",
      "Loss at step 0595: 0.652\n",
      "Loss at step 0596: 0.652\n",
      "Loss at step 0597: 0.659\n",
      "Loss at step 0598: 0.662\n",
      "Loss at step 0599: 0.663\n",
      "Loss at step 0600: 0.660\n",
      "Loss at step 0601: 0.650\n",
      "Loss at step 0602: 0.670\n",
      "Loss at step 0603: 0.666\n",
      "Loss at step 0604: 0.663\n",
      "Loss at step 0605: 0.655\n",
      "Loss at step 0606: 0.649\n",
      "Loss at step 0607: 0.670\n",
      "Loss at step 0608: 0.687\n",
      "Loss at step 0609: 0.676\n",
      "Loss at step 0610: 0.669\n",
      "Loss at step 0611: 0.665\n",
      "Loss at step 0612: 0.668\n",
      "Loss at step 0613: 0.670\n",
      "Loss at step 0614: 0.681\n",
      "Loss at step 0615: 0.651\n",
      "Loss at step 0616: 0.658\n",
      "Loss at step 0617: 0.658\n",
      "Loss at step 0618: 0.656\n",
      "Loss at step 0619: 0.662\n",
      "Loss at step 0620: 0.662\n",
      "Loss at step 0621: 0.663\n",
      "Loss at step 0622: 0.669\n",
      "Loss at step 0623: 0.659\n",
      "Loss at step 0624: 0.659\n",
      "Loss at step 0625: 0.668\n",
      "Loss at step 0626: 0.652\n",
      "Loss at step 0627: 0.657\n",
      "Loss at step 0628: 0.653\n",
      "Loss at step 0629: 0.656\n",
      "Loss at step 0630: 0.663\n",
      "Loss at step 0631: 0.662\n",
      "Loss at step 0632: 0.663\n",
      "Loss at step 0633: 0.667\n",
      "Loss at step 0634: 0.667\n",
      "Loss at step 0635: 0.668\n",
      "Loss at step 0636: 0.670\n",
      "Loss at step 0637: 0.670\n",
      "Loss at step 0638: 0.668\n",
      "Loss at step 0639: 0.668\n",
      "Loss at step 0640: 0.670\n",
      "Loss at step 0641: 0.643\n",
      "Loss at step 0642: 0.664\n",
      "Loss at step 0643: 0.653\n",
      "Loss at step 0644: 0.649\n",
      "Loss at step 0645: 0.647\n",
      "Loss at step 0646: 0.684\n",
      "Loss at step 0647: 0.640\n",
      "Loss at step 0648: 0.635\n",
      "Loss at step 0649: 0.665\n",
      "Loss at step 0650: 0.667\n",
      "Loss at step 0651: 0.674\n",
      "Loss at step 0652: 0.661\n",
      "Loss at step 0653: 0.661\n",
      "Loss at step 0654: 0.663\n",
      "Loss at step 0655: 0.661\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 0656: 0.667\n",
      "Loss at step 0657: 0.662\n",
      "Loss at step 0658: 0.681\n",
      "Loss at step 0659: 0.642\n",
      "Loss at step 0660: 0.645\n",
      "Loss at step 0661: 0.647\n",
      "Loss at step 0662: 0.658\n",
      "Loss at step 0663: 0.659\n",
      "Loss at step 0664: 0.645\n",
      "Loss at step 0665: 0.655\n",
      "Loss at step 0666: 0.642\n",
      "Loss at step 0667: 0.642\n",
      "Loss at step 0668: 0.642\n",
      "Loss at step 0669: 0.622\n",
      "Loss at step 0670: 0.656\n",
      "Loss at step 0671: 0.640\n",
      "Loss at step 0672: 0.644\n",
      "Loss at step 0673: 0.632\n",
      "Loss at step 0674: 0.632\n",
      "Loss at step 0675: 0.628\n",
      "Loss at step 0676: 0.633\n",
      "Loss at step 0677: 0.636\n",
      "Loss at step 0678: 0.628\n",
      "Loss at step 0679: 0.634\n",
      "Loss at step 0680: 0.628\n",
      "Loss at step 0681: 0.641\n",
      "Loss at step 0682: 0.644\n",
      "Loss at step 0683: 0.643\n",
      "Loss at step 0684: 0.624\n",
      "Loss at step 0685: 0.622\n",
      "Loss at step 0686: 0.619\n",
      "Loss at step 0687: 0.620\n",
      "Loss at step 0688: 0.620\n",
      "Loss at step 0689: 0.637\n",
      "Loss at step 0690: 0.645\n",
      "Loss at step 0691: 0.648\n",
      "Loss at step 0692: 0.647\n",
      "Loss at step 0693: 0.649\n",
      "Loss at step 0694: 0.636\n",
      "Loss at step 0695: 0.643\n",
      "Loss at step 0696: 0.652\n",
      "Loss at step 0697: 0.660\n",
      "Loss at step 0698: 0.653\n",
      "Loss at step 0699: 0.646\n",
      "Loss at step 0700: 0.657\n",
      "Loss at step 0701: 0.636\n",
      "Loss at step 0702: 0.640\n",
      "Loss at step 0703: 0.645\n",
      "Loss at step 0704: 0.646\n",
      "Loss at step 0705: 0.642\n",
      "Loss at step 0706: 0.641\n",
      "Loss at step 0707: 0.636\n",
      "Loss at step 0708: 0.649\n",
      "Loss at step 0709: 0.646\n",
      "Loss at step 0710: 0.642\n",
      "Loss at step 0711: 0.640\n",
      "Loss at step 0712: 0.635\n",
      "Loss at step 0713: 0.646\n",
      "Loss at step 0714: 0.642\n",
      "Loss at step 0715: 0.640\n",
      "Loss at step 0716: 0.629\n",
      "Loss at step 0717: 0.626\n",
      "Loss at step 0718: 0.626\n",
      "Loss at step 0719: 0.643\n",
      "Loss at step 0720: 0.633\n",
      "Loss at step 0721: 0.625\n",
      "Loss at step 0722: 0.635\n",
      "Loss at step 0723: 0.621\n",
      "Loss at step 0724: 0.658\n",
      "Loss at step 0725: 0.656\n",
      "Loss at step 0726: 0.656\n",
      "Loss at step 0727: 0.651\n",
      "Loss at step 0728: 0.648\n",
      "Loss at step 0729: 0.647\n",
      "Loss at step 0730: 0.638\n",
      "Loss at step 0731: 0.619\n",
      "Loss at step 0732: 0.625\n",
      "Loss at step 0733: 0.623\n",
      "Loss at step 0734: 0.631\n",
      "Loss at step 0735: 0.622\n",
      "Loss at step 0736: 0.629\n",
      "Loss at step 0737: 0.614\n",
      "Loss at step 0738: 0.605\n",
      "Loss at step 0739: 0.603\n",
      "Loss at step 0740: 0.650\n",
      "Loss at step 0741: 0.651\n",
      "Loss at step 0742: 0.642\n",
      "Loss at step 0743: 0.628\n",
      "Loss at step 0744: 0.614\n",
      "Loss at step 0745: 0.629\n",
      "Loss at step 0746: 0.626\n",
      "Loss at step 0747: 0.611\n",
      "Loss at step 0748: 0.636\n",
      "Loss at step 0749: 0.630\n",
      "Loss at step 0750: 0.629\n",
      "Loss at step 0751: 0.619\n",
      "Loss at step 0752: 0.624\n",
      "Loss at step 0753: 0.633\n",
      "Loss at step 0754: 0.642\n",
      "Loss at step 0755: 0.648\n",
      "Loss at step 0756: 0.646\n",
      "Loss at step 0757: 0.634\n",
      "Loss at step 0758: 0.635\n",
      "Loss at step 0759: 0.633\n",
      "Loss at step 0760: 0.634\n",
      "Loss at step 0761: 0.640\n",
      "Loss at step 0762: 0.638\n",
      "Loss at step 0763: 0.647\n",
      "Loss at step 0764: 0.656\n",
      "Loss at step 0765: 0.652\n",
      "Loss at step 0766: 0.652\n",
      "Loss at step 0767: 0.663\n",
      "Loss at step 0768: 0.643\n",
      "Loss at step 0769: 0.642\n",
      "Loss at step 0770: 0.648\n",
      "Loss at step 0771: 0.638\n",
      "Loss at step 0772: 0.659\n",
      "Loss at step 0773: 0.644\n",
      "Loss at step 0774: 0.637\n",
      "Loss at step 0775: 0.629\n",
      "Loss at step 0776: 0.665\n",
      "Loss at step 0777: 0.651\n",
      "Loss at step 0778: 0.638\n",
      "Loss at step 0779: 0.660\n",
      "Loss at step 0780: 0.685\n",
      "Loss at step 0781: 0.662\n",
      "Loss at step 0782: 0.665\n",
      "Loss at step 0783: 0.645\n",
      "Loss at step 0784: 0.636\n",
      "Loss at step 0785: 0.653\n",
      "Loss at step 0786: 0.627\n",
      "Loss at step 0787: 0.632\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-0fbd3aae1d2b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m# for epoch in range(epochs):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/ml-team/homes/rafal.jakubanis/envs/python3v/lib/python3.5/site-packages/tensorflow/python/keras/_impl/keras/preprocessing/image.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    955\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    956\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 957\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    958\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    959\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_get_batches_of_transformed_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/ml-team/homes/rafal.jakubanis/envs/python3v/lib/python3.5/site-packages/tensorflow/python/keras/_impl/keras/preprocessing/image.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1398\u001b[0m     \u001b[0;31m# The transformation of images is not under thread lock\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1399\u001b[0m     \u001b[0;31m# so it can be done in parallel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1400\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_batches_of_transformed_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/mnt/ml-team/homes/rafal.jakubanis/envs/python3v/lib/python3.5/site-packages/tensorflow/python/keras/_impl/keras/preprocessing/image.py\u001b[0m in \u001b[0;36m_get_batches_of_transformed_samples\u001b[0;34m(self, index_array)\u001b[0m\n\u001b[1;32m   1360\u001b[0m           interpolation=self.interpolation)\n\u001b[1;32m   1361\u001b[0m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg_to_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1362\u001b[0;31m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_data_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1363\u001b[0m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_data_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstandardize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m       \u001b[0mbatch_x\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/ml-team/homes/rafal.jakubanis/envs/python3v/lib/python3.5/site-packages/tensorflow/python/keras/_impl/keras/preprocessing/image.py\u001b[0m in \u001b[0;36mrandom_transform\u001b[0;34m(self, x, seed)\u001b[0m\n\u001b[1;32m    792\u001b[0m           \u001b[0mimg_channel_axis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    793\u001b[0m           \u001b[0mfill_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill_mode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 794\u001b[0;31m           cval=self.cval)\n\u001b[0m\u001b[1;32m    795\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    796\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchannel_shift_range\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/ml-team/homes/rafal.jakubanis/envs/python3v/lib/python3.5/site-packages/tensorflow/python/keras/_impl/keras/preprocessing/image.py\u001b[0m in \u001b[0;36mapply_transform\u001b[0;34m(x, transform_matrix, channel_axis, fill_mode, cval)\u001b[0m\n\u001b[1;32m    295\u001b[0m           \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m           \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfill_mode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 297\u001b[0;31m           cval=cval) for x_channel in x\n\u001b[0m\u001b[1;32m    298\u001b[0m   ]\n\u001b[1;32m    299\u001b[0m   \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchannel_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/ml-team/homes/rafal.jakubanis/envs/python3v/lib/python3.5/site-packages/tensorflow/python/keras/_impl/keras/preprocessing/image.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    295\u001b[0m           \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m           \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfill_mode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 297\u001b[0;31m           cval=cval) for x_channel in x\n\u001b[0m\u001b[1;32m    298\u001b[0m   ]\n\u001b[1;32m    299\u001b[0m   \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchannel_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/ml-team/homes/rafal.jakubanis/envs/python3v/lib/python3.5/site-packages/scipy/ndimage/interpolation.py\u001b[0m in \u001b[0;36maffine_transform\u001b[0;34m(input, matrix, offset, output_shape, output, order, mode, cval, prefilter)\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m         _nd_image.geometric_transform(filtered, None, None, matrix, offset,\n\u001b[0;32m--> 458\u001b[0;31m                                       output, order, mode, cval, None, None)\n\u001b[0m\u001b[1;32m    459\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with tf.device(\"/gpu:0\"):\n",
    "\n",
    "    # for epoch in range(epochs):\n",
    "    for (batch, (inputs, labels)) in enumerate(train_generator):\n",
    "\n",
    "        grads = grad(model, inputs, labels)\n",
    "\n",
    "        optimizer.apply_gradients(zip(grads, model.variables),\n",
    "                                    global_step=tf.train.get_or_create_global_step())\n",
    "#         if batch % 200 == 0:\n",
    "#         print(\"Loss at step {:04d}: {:.3f}\".format(batch, loss(model, x, y)))\n",
    "        print(\"Loss at step {:04d}: {:.3f}\".format(batch, loss(model, x, y)))\n",
    "        \n",
    "print(\"Final loss: {:.3f}\".format(loss(model, x, y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the best model and evaluate it on a sample image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loaded_model = keras.models.load_model(model_checkpoint_dir, compile=False) #compile causes issues if not turned off - we are not training the model so we don't need to compile the model #the workaround to this user warning error might be to set all weights to not-trainable explicitly (in a loop) https://stackoverflow.com/questions/49195189/error-loading-the-saved-optimizer-keras-python-raspberry\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cat = \"/mnt/ml-team/homes/rafal.jakubanis/blogpost2-resnet50/data_small/test/cats/cat.1505.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = image.load_img(test_cat, target_size=(img_size, img_size))\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "preds = loaded_model.predict(x)\n",
    "preds\n",
    "# pred_class = loaded_model.predict_classes(x)\n",
    "# pred_class = loaded_model.probas_to_classes(x)\n",
    "\n",
    "if preds[0][0] < 0.5:\n",
    "    print(\"IT'S A CAT! ({:.2})\".format(preds[0][0]))\n",
    "else:\n",
    "    print(\"IT'S A DOG! ({:.2})\".format(preds[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = img.resize((128,128))\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
